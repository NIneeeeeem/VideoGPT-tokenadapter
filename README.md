# ReadMe

==========================================================

Qwen2.5 - 3B å®éªŒ

ä¸‹è½½ [qwen3b](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct) (Qwen/Qwen2.5-3B-Instruct) æƒé‡åˆ° .cache/qwen2_5

é¢„è®­ç»ƒçš„ projector æƒé‡ å·²ä¸Šä¼  [Wangxc1000/qwen2.5_3B_projectors](https://huggingface.co/Wangxc1000/qwen2.5_3B_projectors)


==========================================================

é¢„è®­ç»ƒæ•°æ®åŠè„šæœ¬

+ æ•°æ®ï¼šåœ¨ä¸‹è½½[instruction_tuning](https://huggingface.co/datasets/MBZUAI/VideoGPT-plus_Training_Dataset)æ•°æ®æ—¶ï¼ŒåŒ…å«äº†é¢„è®­ç»ƒçš„å›¾ç‰‡æ•°æ®,åœ¨pretrainæ–‡ä»¶å¤¹ä¸­ï¼Œè§£å‹å³å¯ï¼ˆè¿™æ¬¡æ²¡åˆ æ•°æ®ï¼‰

+ è„šæœ¬
```
bash  scripts/pretrain_projector_image_encoder.sh

bash  scripts/pretrain_projector_video_encoder.sh
```
```
VideoGPT-plus
â”œâ”€ .cache
â”‚  â”œâ”€ instruction_data
â”‚     â”œâ”€ pretraining
â”‚        â”œâ”€ COCO
â”‚        â”œâ”€ cc3M
```

==========================================================

EXP. VideoGPT+ åœ¨ä¸åŒpoolä¸Šè¡¨ç° llmä½¿ç”¨ qwen2.5 1.5B/~~3B~~

ä»£ç  [https://github.com/NIneeeeeem/VideoGPT-tokenadapter.git](https://github.com/NIneeeeeem/VideoGPT-tokenadapter.git)

å…³äºæ•°æ®ä¸‹è½½ : [huggingface]

MBZUAI/VideoGPT-plus_Training_Dataset ã€åŒ…å«äº†videoså’ŒåŸæœ¬çš„annotionsã€‘

OpenGVLab/MVBench ã€åŒ…å«MVBENCHæ ‡æ³¨å’Œè§†é¢‘ã€‘

æ¨¡å‹æƒé‡ä¸‹è½½ : [huggingface]

openai/clip-vit-large-patch14-336

OpenGVLab/InternVideo2-Stage2_1B-224p-f4

Qwen/Qwen2.5-1.5B-Instruct

Wangxc1000/qwen2.5_1.5B_projectors  [projectoræƒé‡æ–°ç‰ˆ]

~~Wangxc1000/Training_caps_data ã€æ­£åœ¨ä¸Šä¼ ã€‘åŒ…å«captionæ ‡æ³¨çš„ä¿¡æ¯~~

```python
export HF_ENDPOINT=https://hf-mirror.com
bash hfd.sh MBZUAI/VideoGPT-plus_Training_Dataset
```

å…³äºç¯å¢ƒé…ç½®ï¼š

torch å®éªŒå®¤ 2.4.0+cu118

éœ€è¦å•ç‹¬ä¸‹è½½ flash-attn ~~ï¼ˆå®éªŒå®¤ç‰ˆæœ¬2.6.3ï¼‰~~ pip install flash-attn --no-build-isolation

å…¶ä»– pip install -r requirements.txt å³å¯

Pretrain è„šæœ¬åœ¨

scripts/pretrain_projector_image_encoder.sh

scripts/pretrain_projector_video_encoder.sh

å·²ç¡®å®š VideoGPT+ çš„bsè®¾ç½®ä¸º256

è®­ç»ƒè„šæœ¬ä½ç½® scripts/pool_full

éœ€è¦ä¿®æ”¹  export PYTHONPATH="/hhd2/wxc/VideoGPT-plus:$PYTHONPATHâ€

pool å®ç°é€»è¾‘ï¼š

```bash
def divide_and_round(a, b):
            result = a / b
            if result < 1:
                return 1
            else:
                return round(result)
video_feature: shape=(divide_and_round(16, pool_level), divide_and_round(16, pool_level)))
image_feature: shape=(divide_and_round(24, pool_level), divide_and_round(24, pool_level))
```

pool1 å³ b=1 è¡¨ç¤ºä¸è¿›è¡Œ pool

æµ‹è¯•è„šæœ¬ä½ç½® scripts/eval_pool_full

ã€ä»£ç å¾…ä¿®æ”¹ï¼Œé¢„è®¡ä¸‹åˆæ”¹å®Œã€‘




## Installation :wrench:

We recommend setting up a conda environment for the project:
```shell
conda create --name=videogpt_plus python=3.11
conda activate videogpt_plus

git clone https://github.com/mbzuai-oryx/VideoGPT-plus
cd VideoGPT-plus

pip install torch==2.1.2 torchvision==0.16.2 --index-url https://download.pytorch.org/whl/cu118
pip install transformers==4.41.0

pip install -r requirements.txt

export PYTHONPATH="./:$PYTHONPATH"
```
Additionally, install [FlashAttention](https://github.com/HazyResearch/flash-attention) for training,
```shell
pip install ninja

git clone https://github.com/HazyResearch/flash-attention.git
cd flash-attention
python setup.py install
```
---

## Quantitative Evaluation ğŸ“Š
We provide instructions to reproduce VideoGPT+ results on VCGBench, VCGBench-Diverse and MVBench. Please follow the instructions at [eval/README.md](eval/README.md).

### VCGBench Evaluation: Video-based Generative Performance Benchmarking :chart_with_upwards_trend:
<p align="center">
  <img src="docs/images/VCGBench_quantitative.png" alt="VCGBench_quantitative" width="1000">
</p>

---
### VCGBench-Diverse Evaluation :bar_chart:
<p align="center">
  <img src="docs/images/VCGDiverse_quantitative.png" alt="VCGDiverse_quantitative">
</p>

---
### Zero-Shot Question-Answer Evaluation :question:
<p align="center">
  <img src="docs/images/zero_shot_quantitative.png" alt="zero_shot_quantitative">
</p>

---

### MVBench Evaluation :movie_camera:
<p align="center">
  <img src="docs/images/MVBench_quantitative.png" alt="MVBench_quantitative">
</p>

---

## Training :train:
We provide scripts for pretraining and finetuning of VideoGPT+. Please follow the instructions at [scripts/README.md](scripts/README.md).

---

## Qualitative Analysis :mag:
A comprehensive evaluation of VideoGPT+ performance across multiple tasks and domains.
<p align="center">
  <img src="docs/images/demo_vcg+_main.png" alt="demo_vcg+_main" width="700">
</p>

---

<p align="center">
  <img src="docs/images/demo_vcg+_full_part1.jpg" alt="demo_vcg+_full_part1" width="700">
</p>


<p align="center">
  <img src="docs/images/demo_vcg+_full_part2.jpg" alt="demo_vcg+_full_part2" width="700">
</p>

---

## Acknowledgements :pray:

+ [Video-ChatGPT](https://github.com/mbzuai-oryx/Video-ChatGPT): A pioneering attempt in Video-based conversation models.
+ [LLaVA](https://github.com/haotian-liu/LLaVA): Our code base is build upon LLaVA and Video-ChatGPT.
+ [Chat-UniVi](https://github.com/PKU-YuanGroup/Chat-UniVi): A recent work in image and video-based conversation models. We borrowed some implementation details from their public codebase.

## Citations ğŸ“œ:

If you're using VideoGPT+ in your research or applications, please cite using this BibTeX:
```bibtex
@article{Maaz2024VideoGPT+,
    title={VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding},
    author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz},
    journal={arxiv},
    year={2024},
    url={https://arxiv.org/abs/2406.09418}
}

@inproceedings{Maaz2023VideoChatGPT,
    title={Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models},
    author={Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz},
    booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024)},
    year={2024}
}
```

## License :scroll:
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.


Looking forward to your feedback, contributions, and stars! :star2:
Please raise any issues or questions [here](https://github.com/mbzuai-oryx/VideoGPT-plus/issues). 


---
[<img src="docs/images/IVAL_logo.png" width="200" height="100">](https://www.ival-mbzuai.com)
[<img src="docs/images/Oryx_logo.png" width="100" height="100">](https://github.com/mbzuai-oryx)
[<img src="docs/images/MBZUAI_logo.png" width="360" height="85">](https://mbzuai.ac.ae)