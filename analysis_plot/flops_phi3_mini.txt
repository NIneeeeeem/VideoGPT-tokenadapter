09/25 14:03:29 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
09/25 14:03:29 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
Not using distributed mode
Loading VideoGPT+ from base model...
Building OpenGVLab/InternVideo2-Stage2_1B-224p-f4/InternVideo2-stage2_1b-224p-f4.pt
load_state_dict: _IncompatibleKeys(missing_keys=[], unexpected_keys=['temp', 'text_encoder.bert.embeddings.position_ids', 'text_encoder.bert.embeddings.word_embeddings.weight', 'text_encoder.bert.embeddings.position_embeddings.weight', 'text_encoder.bert.embeddings.token_type_embeddings.weight', 'text_encoder.bert.embeddings.LayerNorm.weight', 'text_encoder.bert.embeddings.LayerNorm.bias', 'text_encoder.bert.encoder.layer.0.attention.self.query.weight', 'text_encoder.bert.encoder.layer.0.attention.self.query.bias', 'text_encoder.bert.encoder.layer.0.attention.self.key.weight', 'text_encoder.bert.encoder.layer.0.attention.self.key.bias', 'text_encoder.bert.encoder.layer.0.attention.self.value.weight', 'text_encoder.bert.encoder.layer.0.attention.self.value.bias', 'text_encoder.bert.encoder.layer.0.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.0.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.0.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.0.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.0.output.dense.weight', 'text_encoder.bert.encoder.layer.0.output.dense.bias', 'text_encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.1.attention.self.query.weight', 'text_encoder.bert.encoder.layer.1.attention.self.query.bias', 'text_encoder.bert.encoder.layer.1.attention.self.key.weight', 'text_encoder.bert.encoder.layer.1.attention.self.key.bias', 'text_encoder.bert.encoder.layer.1.attention.self.value.weight', 'text_encoder.bert.encoder.layer.1.attention.self.value.bias', 'text_encoder.bert.encoder.layer.1.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.1.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.1.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.1.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.1.output.dense.weight', 'text_encoder.bert.encoder.layer.1.output.dense.bias', 'text_encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.2.attention.self.query.weight', 'text_encoder.bert.encoder.layer.2.attention.self.query.bias', 'text_encoder.bert.encoder.layer.2.attention.self.key.weight', 'text_encoder.bert.encoder.layer.2.attention.self.key.bias', 'text_encoder.bert.encoder.layer.2.attention.self.value.weight', 'text_encoder.bert.encoder.layer.2.attention.self.value.bias', 'text_encoder.bert.encoder.layer.2.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.2.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.2.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.2.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.2.output.dense.weight', 'text_encoder.bert.encoder.layer.2.output.dense.bias', 'text_encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.3.attention.self.query.weight', 'text_encoder.bert.encoder.layer.3.attention.self.query.bias', 'text_encoder.bert.encoder.layer.3.attention.self.key.weight', 'text_encoder.bert.encoder.layer.3.attention.self.key.bias', 'text_encoder.bert.encoder.layer.3.attention.self.value.weight', 'text_encoder.bert.encoder.layer.3.attention.self.value.bias', 'text_encoder.bert.encoder.layer.3.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.3.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.3.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.3.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.3.output.dense.weight', 'text_encoder.bert.encoder.layer.3.output.dense.bias', 'text_encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.4.attention.self.query.weight', 'text_encoder.bert.encoder.layer.4.attention.self.query.bias', 'text_encoder.bert.encoder.layer.4.attention.self.key.weight', 'text_encoder.bert.encoder.layer.4.attention.self.key.bias', 'text_encoder.bert.encoder.layer.4.attention.self.value.weight', 'text_encoder.bert.encoder.layer.4.attention.self.value.bias', 'text_encoder.bert.encoder.layer.4.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.4.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.4.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.4.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.4.output.dense.weight', 'text_encoder.bert.encoder.layer.4.output.dense.bias', 'text_encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.5.attention.self.query.weight', 'text_encoder.bert.encoder.layer.5.attention.self.query.bias', 'text_encoder.bert.encoder.layer.5.attention.self.key.weight', 'text_encoder.bert.encoder.layer.5.attention.self.key.bias', 'text_encoder.bert.encoder.layer.5.attention.self.value.weight', 'text_encoder.bert.encoder.layer.5.attention.self.value.bias', 'text_encoder.bert.encoder.layer.5.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.5.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.5.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.5.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.5.output.dense.weight', 'text_encoder.bert.encoder.layer.5.output.dense.bias', 'text_encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.6.attention.self.query.weight', 'text_encoder.bert.encoder.layer.6.attention.self.query.bias', 'text_encoder.bert.encoder.layer.6.attention.self.key.weight', 'text_encoder.bert.encoder.layer.6.attention.self.key.bias', 'text_encoder.bert.encoder.layer.6.attention.self.value.weight', 'text_encoder.bert.encoder.layer.6.attention.self.value.bias', 'text_encoder.bert.encoder.layer.6.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.6.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.6.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.6.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.6.output.dense.weight', 'text_encoder.bert.encoder.layer.6.output.dense.bias', 'text_encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.7.attention.self.query.weight', 'text_encoder.bert.encoder.layer.7.attention.self.query.bias', 'text_encoder.bert.encoder.layer.7.attention.self.key.weight', 'text_encoder.bert.encoder.layer.7.attention.self.key.bias', 'text_encoder.bert.encoder.layer.7.attention.self.value.weight', 'text_encoder.bert.encoder.layer.7.attention.self.value.bias', 'text_encoder.bert.encoder.layer.7.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.7.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.7.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.7.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.7.output.dense.weight', 'text_encoder.bert.encoder.layer.7.output.dense.bias', 'text_encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.8.attention.self.query.weight', 'text_encoder.bert.encoder.layer.8.attention.self.query.bias', 'text_encoder.bert.encoder.layer.8.attention.self.key.weight', 'text_encoder.bert.encoder.layer.8.attention.self.key.bias', 'text_encoder.bert.encoder.layer.8.attention.self.value.weight', 'text_encoder.bert.encoder.layer.8.attention.self.value.bias', 'text_encoder.bert.encoder.layer.8.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.8.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.8.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.8.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.8.output.dense.weight', 'text_encoder.bert.encoder.layer.8.output.dense.bias', 'text_encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.9.attention.self.query.weight', 'text_encoder.bert.encoder.layer.9.attention.self.query.bias', 'text_encoder.bert.encoder.layer.9.attention.self.key.weight', 'text_encoder.bert.encoder.layer.9.attention.self.key.bias', 'text_encoder.bert.encoder.layer.9.attention.self.value.weight', 'text_encoder.bert.encoder.layer.9.attention.self.value.bias', 'text_encoder.bert.encoder.layer.9.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.9.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.9.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.9.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.9.output.dense.weight', 'text_encoder.bert.encoder.layer.9.output.dense.bias', 'text_encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.10.attention.self.query.weight', 'text_encoder.bert.encoder.layer.10.attention.self.query.bias', 'text_encoder.bert.encoder.layer.10.attention.self.key.weight', 'text_encoder.bert.encoder.layer.10.attention.self.key.bias', 'text_encoder.bert.encoder.layer.10.attention.self.value.weight', 'text_encoder.bert.encoder.layer.10.attention.self.value.bias', 'text_encoder.bert.encoder.layer.10.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.10.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.10.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.10.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.10.output.dense.weight', 'text_encoder.bert.encoder.layer.10.output.dense.bias', 'text_encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.11.attention.self.query.weight', 'text_encoder.bert.encoder.layer.11.attention.self.query.bias', 'text_encoder.bert.encoder.layer.11.attention.self.key.weight', 'text_encoder.bert.encoder.layer.11.attention.self.key.bias', 'text_encoder.bert.encoder.layer.11.attention.self.value.weight', 'text_encoder.bert.encoder.layer.11.attention.self.value.bias', 'text_encoder.bert.encoder.layer.11.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.11.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.11.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.11.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.11.output.dense.weight', 'text_encoder.bert.encoder.layer.11.output.dense.bias', 'text_encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.12.attention.self.query.weight', 'text_encoder.bert.encoder.layer.12.attention.self.query.bias', 'text_encoder.bert.encoder.layer.12.attention.self.key.weight', 'text_encoder.bert.encoder.layer.12.attention.self.key.bias', 'text_encoder.bert.encoder.layer.12.attention.self.value.weight', 'text_encoder.bert.encoder.layer.12.attention.self.value.bias', 'text_encoder.bert.encoder.layer.12.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.12.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.12.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.12.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.12.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.12.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.12.output.dense.weight', 'text_encoder.bert.encoder.layer.12.output.dense.bias', 'text_encoder.bert.encoder.layer.12.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.12.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.13.attention.self.query.weight', 'text_encoder.bert.encoder.layer.13.attention.self.query.bias', 'text_encoder.bert.encoder.layer.13.attention.self.key.weight', 'text_encoder.bert.encoder.layer.13.attention.self.key.bias', 'text_encoder.bert.encoder.layer.13.attention.self.value.weight', 'text_encoder.bert.encoder.layer.13.attention.self.value.bias', 'text_encoder.bert.encoder.layer.13.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.13.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.13.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.13.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.13.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.13.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.13.output.dense.weight', 'text_encoder.bert.encoder.layer.13.output.dense.bias', 'text_encoder.bert.encoder.layer.13.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.13.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.14.attention.self.query.weight', 'text_encoder.bert.encoder.layer.14.attention.self.query.bias', 'text_encoder.bert.encoder.layer.14.attention.self.key.weight', 'text_encoder.bert.encoder.layer.14.attention.self.key.bias', 'text_encoder.bert.encoder.layer.14.attention.self.value.weight', 'text_encoder.bert.encoder.layer.14.attention.self.value.bias', 'text_encoder.bert.encoder.layer.14.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.14.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.14.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.14.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.14.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.14.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.14.output.dense.weight', 'text_encoder.bert.encoder.layer.14.output.dense.bias', 'text_encoder.bert.encoder.layer.14.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.14.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.15.attention.self.query.weight', 'text_encoder.bert.encoder.layer.15.attention.self.query.bias', 'text_encoder.bert.encoder.layer.15.attention.self.key.weight', 'text_encoder.bert.encoder.layer.15.attention.self.key.bias', 'text_encoder.bert.encoder.layer.15.attention.self.value.weight', 'text_encoder.bert.encoder.layer.15.attention.self.value.bias', 'text_encoder.bert.encoder.layer.15.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.15.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.15.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.15.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.15.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.15.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.15.output.dense.weight', 'text_encoder.bert.encoder.layer.15.output.dense.bias', 'text_encoder.bert.encoder.layer.15.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.15.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.16.attention.self.query.weight', 'text_encoder.bert.encoder.layer.16.attention.self.query.bias', 'text_encoder.bert.encoder.layer.16.attention.self.key.weight', 'text_encoder.bert.encoder.layer.16.attention.self.key.bias', 'text_encoder.bert.encoder.layer.16.attention.self.value.weight', 'text_encoder.bert.encoder.layer.16.attention.self.value.bias', 'text_encoder.bert.encoder.layer.16.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.16.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.16.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.16.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.16.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.16.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.16.output.dense.weight', 'text_encoder.bert.encoder.layer.16.output.dense.bias', 'text_encoder.bert.encoder.layer.16.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.16.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.17.attention.self.query.weight', 'text_encoder.bert.encoder.layer.17.attention.self.query.bias', 'text_encoder.bert.encoder.layer.17.attention.self.key.weight', 'text_encoder.bert.encoder.layer.17.attention.self.key.bias', 'text_encoder.bert.encoder.layer.17.attention.self.value.weight', 'text_encoder.bert.encoder.layer.17.attention.self.value.bias', 'text_encoder.bert.encoder.layer.17.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.17.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.17.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.17.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.17.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.17.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.17.output.dense.weight', 'text_encoder.bert.encoder.layer.17.output.dense.bias', 'text_encoder.bert.encoder.layer.17.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.17.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.18.attention.self.query.weight', 'text_encoder.bert.encoder.layer.18.attention.self.query.bias', 'text_encoder.bert.encoder.layer.18.attention.self.key.weight', 'text_encoder.bert.encoder.layer.18.attention.self.key.bias', 'text_encoder.bert.encoder.layer.18.attention.self.value.weight', 'text_encoder.bert.encoder.layer.18.attention.self.value.bias', 'text_encoder.bert.encoder.layer.18.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.18.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.18.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.18.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.18.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.18.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.18.output.dense.weight', 'text_encoder.bert.encoder.layer.18.output.dense.bias', 'text_encoder.bert.encoder.layer.18.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.18.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.19.attention.self.query.weight', 'text_encoder.bert.encoder.layer.19.attention.self.query.bias', 'text_encoder.bert.encoder.layer.19.attention.self.key.weight', 'text_encoder.bert.encoder.layer.19.attention.self.key.bias', 'text_encoder.bert.encoder.layer.19.attention.self.value.weight', 'text_encoder.bert.encoder.layer.19.attention.self.value.bias', 'text_encoder.bert.encoder.layer.19.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.19.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.19.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.19.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.19.crossattention.self.query.weight', 'text_encoder.bert.encoder.layer.19.crossattention.self.query.bias', 'text_encoder.bert.encoder.layer.19.crossattention.self.key.weight', 'text_encoder.bert.encoder.layer.19.crossattention.self.key.bias', 'text_encoder.bert.encoder.layer.19.crossattention.self.value.weight', 'text_encoder.bert.encoder.layer.19.crossattention.self.value.bias', 'text_encoder.bert.encoder.layer.19.crossattention.output.dense.weight', 'text_encoder.bert.encoder.layer.19.crossattention.output.dense.bias', 'text_encoder.bert.encoder.layer.19.crossattention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.19.crossattention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.19.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.19.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.19.output.dense.weight', 'text_encoder.bert.encoder.layer.19.output.dense.bias', 'text_encoder.bert.encoder.layer.19.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.19.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.20.attention.self.query.weight', 'text_encoder.bert.encoder.layer.20.attention.self.query.bias', 'text_encoder.bert.encoder.layer.20.attention.self.key.weight', 'text_encoder.bert.encoder.layer.20.attention.self.key.bias', 'text_encoder.bert.encoder.layer.20.attention.self.value.weight', 'text_encoder.bert.encoder.layer.20.attention.self.value.bias', 'text_encoder.bert.encoder.layer.20.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.20.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.20.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.20.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.20.crossattention.self.query.weight', 'text_encoder.bert.encoder.layer.20.crossattention.self.query.bias', 'text_encoder.bert.encoder.layer.20.crossattention.self.key.weight', 'text_encoder.bert.encoder.layer.20.crossattention.self.key.bias', 'text_encoder.bert.encoder.layer.20.crossattention.self.value.weight', 'text_encoder.bert.encoder.layer.20.crossattention.self.value.bias', 'text_encoder.bert.encoder.layer.20.crossattention.output.dense.weight', 'text_encoder.bert.encoder.layer.20.crossattention.output.dense.bias', 'text_encoder.bert.encoder.layer.20.crossattention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.20.crossattention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.20.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.20.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.20.output.dense.weight', 'text_encoder.bert.encoder.layer.20.output.dense.bias', 'text_encoder.bert.encoder.layer.20.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.20.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.21.attention.self.query.weight', 'text_encoder.bert.encoder.layer.21.attention.self.query.bias', 'text_encoder.bert.encoder.layer.21.attention.self.key.weight', 'text_encoder.bert.encoder.layer.21.attention.self.key.bias', 'text_encoder.bert.encoder.layer.21.attention.self.value.weight', 'text_encoder.bert.encoder.layer.21.attention.self.value.bias', 'text_encoder.bert.encoder.layer.21.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.21.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.21.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.21.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.21.crossattention.self.query.weight', 'text_encoder.bert.encoder.layer.21.crossattention.self.query.bias', 'text_encoder.bert.encoder.layer.21.crossattention.self.key.weight', 'text_encoder.bert.encoder.layer.21.crossattention.self.key.bias', 'text_encoder.bert.encoder.layer.21.crossattention.self.value.weight', 'text_encoder.bert.encoder.layer.21.crossattention.self.value.bias', 'text_encoder.bert.encoder.layer.21.crossattention.output.dense.weight', 'text_encoder.bert.encoder.layer.21.crossattention.output.dense.bias', 'text_encoder.bert.encoder.layer.21.crossattention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.21.crossattention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.21.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.21.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.21.output.dense.weight', 'text_encoder.bert.encoder.layer.21.output.dense.bias', 'text_encoder.bert.encoder.layer.21.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.21.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.22.attention.self.query.weight', 'text_encoder.bert.encoder.layer.22.attention.self.query.bias', 'text_encoder.bert.encoder.layer.22.attention.self.key.weight', 'text_encoder.bert.encoder.layer.22.attention.self.key.bias', 'text_encoder.bert.encoder.layer.22.attention.self.value.weight', 'text_encoder.bert.encoder.layer.22.attention.self.value.bias', 'text_encoder.bert.encoder.layer.22.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.22.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.22.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.22.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.22.crossattention.self.query.weight', 'text_encoder.bert.encoder.layer.22.crossattention.self.query.bias', 'text_encoder.bert.encoder.layer.22.crossattention.self.key.weight', 'text_encoder.bert.encoder.layer.22.crossattention.self.key.bias', 'text_encoder.bert.encoder.layer.22.crossattention.self.value.weight', 'text_encoder.bert.encoder.layer.22.crossattention.self.value.bias', 'text_encoder.bert.encoder.layer.22.crossattention.output.dense.weight', 'text_encoder.bert.encoder.layer.22.crossattention.output.dense.bias', 'text_encoder.bert.encoder.layer.22.crossattention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.22.crossattention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.22.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.22.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.22.output.dense.weight', 'text_encoder.bert.encoder.layer.22.output.dense.bias', 'text_encoder.bert.encoder.layer.22.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.22.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.23.attention.self.query.weight', 'text_encoder.bert.encoder.layer.23.attention.self.query.bias', 'text_encoder.bert.encoder.layer.23.attention.self.key.weight', 'text_encoder.bert.encoder.layer.23.attention.self.key.bias', 'text_encoder.bert.encoder.layer.23.attention.self.value.weight', 'text_encoder.bert.encoder.layer.23.attention.self.value.bias', 'text_encoder.bert.encoder.layer.23.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.23.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.23.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.23.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.23.crossattention.self.query.weight', 'text_encoder.bert.encoder.layer.23.crossattention.self.query.bias', 'text_encoder.bert.encoder.layer.23.crossattention.self.key.weight', 'text_encoder.bert.encoder.layer.23.crossattention.self.key.bias', 'text_encoder.bert.encoder.layer.23.crossattention.self.value.weight', 'text_encoder.bert.encoder.layer.23.crossattention.self.value.bias', 'text_encoder.bert.encoder.layer.23.crossattention.output.dense.weight', 'text_encoder.bert.encoder.layer.23.crossattention.output.dense.bias', 'text_encoder.bert.encoder.layer.23.crossattention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.23.crossattention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.23.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.23.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.23.output.dense.weight', 'text_encoder.bert.encoder.layer.23.output.dense.bias', 'text_encoder.bert.encoder.layer.23.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.23.output.LayerNorm.bias', 'text_encoder.cls.predictions.bias', 'text_encoder.cls.predictions.transform.dense.weight', 'text_encoder.cls.predictions.transform.dense.bias', 'text_encoder.cls.predictions.transform.LayerNorm.weight', 'text_encoder.cls.predictions.transform.LayerNorm.bias', 'text_encoder.cls.predictions.decoder.weight', 'text_encoder.cls.predictions.decoder.bias', 'text_proj.weight', 'text_proj.bias', 'itm_head.weight', 'itm_head.bias'])
Building openai/clip-vit-large-patch14-336
Building mlp2x_gelu
projector_type: mlp2x_gelu
Building mlp2x_gelu
projector_type: mlp2x_gelu
Loading additional VideoGPT+ weights...
Loading LoRA weights...
Merging LoRA weights...
Model is loaded...
load_state_dict: _IncompatibleKeys(missing_keys=[], unexpected_keys=['temp', 'text_encoder.bert.embeddings.position_ids', 'text_encoder.bert.embeddings.word_embeddings.weight', 'text_encoder.bert.embeddings.position_embeddings.weight', 'text_encoder.bert.embeddings.token_type_embeddings.weight', 'text_encoder.bert.embeddings.LayerNorm.weight', 'text_encoder.bert.embeddings.LayerNorm.bias', 'text_encoder.bert.encoder.layer.0.attention.self.query.weight', 'text_encoder.bert.encoder.layer.0.attention.self.query.bias', 'text_encoder.bert.encoder.layer.0.attention.self.key.weight', 'text_encoder.bert.encoder.layer.0.attention.self.key.bias', 'text_encoder.bert.encoder.layer.0.attention.self.value.weight', 'text_encoder.bert.encoder.layer.0.attention.self.value.bias', 'text_encoder.bert.encoder.layer.0.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.0.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.0.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.0.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.0.output.dense.weight', 'text_encoder.bert.encoder.layer.0.output.dense.bias', 'text_encoder.bert.encoder.layer.0.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.0.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.1.attention.self.query.weight', 'text_encoder.bert.encoder.layer.1.attention.self.query.bias', 'text_encoder.bert.encoder.layer.1.attention.self.key.weight', 'text_encoder.bert.encoder.layer.1.attention.self.key.bias', 'text_encoder.bert.encoder.layer.1.attention.self.value.weight', 'text_encoder.bert.encoder.layer.1.attention.self.value.bias', 'text_encoder.bert.encoder.layer.1.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.1.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.1.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.1.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.1.output.dense.weight', 'text_encoder.bert.encoder.layer.1.output.dense.bias', 'text_encoder.bert.encoder.layer.1.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.1.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.2.attention.self.query.weight', 'text_encoder.bert.encoder.layer.2.attention.self.query.bias', 'text_encoder.bert.encoder.layer.2.attention.self.key.weight', 'text_encoder.bert.encoder.layer.2.attention.self.key.bias', 'text_encoder.bert.encoder.layer.2.attention.self.value.weight', 'text_encoder.bert.encoder.layer.2.attention.self.value.bias', 'text_encoder.bert.encoder.layer.2.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.2.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.2.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.2.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.2.output.dense.weight', 'text_encoder.bert.encoder.layer.2.output.dense.bias', 'text_encoder.bert.encoder.layer.2.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.2.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.3.attention.self.query.weight', 'text_encoder.bert.encoder.layer.3.attention.self.query.bias', 'text_encoder.bert.encoder.layer.3.attention.self.key.weight', 'text_encoder.bert.encoder.layer.3.attention.self.key.bias', 'text_encoder.bert.encoder.layer.3.attention.self.value.weight', 'text_encoder.bert.encoder.layer.3.attention.self.value.bias', 'text_encoder.bert.encoder.layer.3.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.3.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.3.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.3.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.3.output.dense.weight', 'text_encoder.bert.encoder.layer.3.output.dense.bias', 'text_encoder.bert.encoder.layer.3.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.3.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.4.attention.self.query.weight', 'text_encoder.bert.encoder.layer.4.attention.self.query.bias', 'text_encoder.bert.encoder.layer.4.attention.self.key.weight', 'text_encoder.bert.encoder.layer.4.attention.self.key.bias', 'text_encoder.bert.encoder.layer.4.attention.self.value.weight', 'text_encoder.bert.encoder.layer.4.attention.self.value.bias', 'text_encoder.bert.encoder.layer.4.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.4.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.4.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.4.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.4.output.dense.weight', 'text_encoder.bert.encoder.layer.4.output.dense.bias', 'text_encoder.bert.encoder.layer.4.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.4.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.5.attention.self.query.weight', 'text_encoder.bert.encoder.layer.5.attention.self.query.bias', 'text_encoder.bert.encoder.layer.5.attention.self.key.weight', 'text_encoder.bert.encoder.layer.5.attention.self.key.bias', 'text_encoder.bert.encoder.layer.5.attention.self.value.weight', 'text_encoder.bert.encoder.layer.5.attention.self.value.bias', 'text_encoder.bert.encoder.layer.5.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.5.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.5.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.5.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.5.output.dense.weight', 'text_encoder.bert.encoder.layer.5.output.dense.bias', 'text_encoder.bert.encoder.layer.5.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.5.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.6.attention.self.query.weight', 'text_encoder.bert.encoder.layer.6.attention.self.query.bias', 'text_encoder.bert.encoder.layer.6.attention.self.key.weight', 'text_encoder.bert.encoder.layer.6.attention.self.key.bias', 'text_encoder.bert.encoder.layer.6.attention.self.value.weight', 'text_encoder.bert.encoder.layer.6.attention.self.value.bias', 'text_encoder.bert.encoder.layer.6.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.6.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.6.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.6.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.6.output.dense.weight', 'text_encoder.bert.encoder.layer.6.output.dense.bias', 'text_encoder.bert.encoder.layer.6.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.6.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.7.attention.self.query.weight', 'text_encoder.bert.encoder.layer.7.attention.self.query.bias', 'text_encoder.bert.encoder.layer.7.attention.self.key.weight', 'text_encoder.bert.encoder.layer.7.attention.self.key.bias', 'text_encoder.bert.encoder.layer.7.attention.self.value.weight', 'text_encoder.bert.encoder.layer.7.attention.self.value.bias', 'text_encoder.bert.encoder.layer.7.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.7.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.7.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.7.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.7.output.dense.weight', 'text_encoder.bert.encoder.layer.7.output.dense.bias', 'text_encoder.bert.encoder.layer.7.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.7.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.8.attention.self.query.weight', 'text_encoder.bert.encoder.layer.8.attention.self.query.bias', 'text_encoder.bert.encoder.layer.8.attention.self.key.weight', 'text_encoder.bert.encoder.layer.8.attention.self.key.bias', 'text_encoder.bert.encoder.layer.8.attention.self.value.weight', 'text_encoder.bert.encoder.layer.8.attention.self.value.bias', 'text_encoder.bert.encoder.layer.8.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.8.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.8.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.8.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.8.output.dense.weight', 'text_encoder.bert.encoder.layer.8.output.dense.bias', 'text_encoder.bert.encoder.layer.8.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.8.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.9.attention.self.query.weight', 'text_encoder.bert.encoder.layer.9.attention.self.query.bias', 'text_encoder.bert.encoder.layer.9.attention.self.key.weight', 'text_encoder.bert.encoder.layer.9.attention.self.key.bias', 'text_encoder.bert.encoder.layer.9.attention.self.value.weight', 'text_encoder.bert.encoder.layer.9.attention.self.value.bias', 'text_encoder.bert.encoder.layer.9.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.9.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.9.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.9.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.9.output.dense.weight', 'text_encoder.bert.encoder.layer.9.output.dense.bias', 'text_encoder.bert.encoder.layer.9.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.9.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.10.attention.self.query.weight', 'text_encoder.bert.encoder.layer.10.attention.self.query.bias', 'text_encoder.bert.encoder.layer.10.attention.self.key.weight', 'text_encoder.bert.encoder.layer.10.attention.self.key.bias', 'text_encoder.bert.encoder.layer.10.attention.self.value.weight', 'text_encoder.bert.encoder.layer.10.attention.self.value.bias', 'text_encoder.bert.encoder.layer.10.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.10.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.10.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.10.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.10.output.dense.weight', 'text_encoder.bert.encoder.layer.10.output.dense.bias', 'text_encoder.bert.encoder.layer.10.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.10.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.11.attention.self.query.weight', 'text_encoder.bert.encoder.layer.11.attention.self.query.bias', 'text_encoder.bert.encoder.layer.11.attention.self.key.weight', 'text_encoder.bert.encoder.layer.11.attention.self.key.bias', 'text_encoder.bert.encoder.layer.11.attention.self.value.weight', 'text_encoder.bert.encoder.layer.11.attention.self.value.bias', 'text_encoder.bert.encoder.layer.11.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.11.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.11.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.11.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.11.output.dense.weight', 'text_encoder.bert.encoder.layer.11.output.dense.bias', 'text_encoder.bert.encoder.layer.11.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.11.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.12.attention.self.query.weight', 'text_encoder.bert.encoder.layer.12.attention.self.query.bias', 'text_encoder.bert.encoder.layer.12.attention.self.key.weight', 'text_encoder.bert.encoder.layer.12.attention.self.key.bias', 'text_encoder.bert.encoder.layer.12.attention.self.value.weight', 'text_encoder.bert.encoder.layer.12.attention.self.value.bias', 'text_encoder.bert.encoder.layer.12.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.12.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.12.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.12.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.12.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.12.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.12.output.dense.weight', 'text_encoder.bert.encoder.layer.12.output.dense.bias', 'text_encoder.bert.encoder.layer.12.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.12.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.13.attention.self.query.weight', 'text_encoder.bert.encoder.layer.13.attention.self.query.bias', 'text_encoder.bert.encoder.layer.13.attention.self.key.weight', 'text_encoder.bert.encoder.layer.13.attention.self.key.bias', 'text_encoder.bert.encoder.layer.13.attention.self.value.weight', 'text_encoder.bert.encoder.layer.13.attention.self.value.bias', 'text_encoder.bert.encoder.layer.13.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.13.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.13.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.13.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.13.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.13.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.13.output.dense.weight', 'text_encoder.bert.encoder.layer.13.output.dense.bias', 'text_encoder.bert.encoder.layer.13.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.13.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.14.attention.self.query.weight', 'text_encoder.bert.encoder.layer.14.attention.self.query.bias', 'text_encoder.bert.encoder.layer.14.attention.self.key.weight', 'text_encoder.bert.encoder.layer.14.attention.self.key.bias', 'text_encoder.bert.encoder.layer.14.attention.self.value.weight', 'text_encoder.bert.encoder.layer.14.attention.self.value.bias', 'text_encoder.bert.encoder.layer.14.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.14.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.14.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.14.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.14.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.14.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.14.output.dense.weight', 'text_encoder.bert.encoder.layer.14.output.dense.bias', 'text_encoder.bert.encoder.layer.14.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.14.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.15.attention.self.query.weight', 'text_encoder.bert.encoder.layer.15.attention.self.query.bias', 'text_encoder.bert.encoder.layer.15.attention.self.key.weight', 'text_encoder.bert.encoder.layer.15.attention.self.key.bias', 'text_encoder.bert.encoder.layer.15.attention.self.value.weight', 'text_encoder.bert.encoder.layer.15.attention.self.value.bias', 'text_encoder.bert.encoder.layer.15.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.15.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.15.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.15.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.15.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.15.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.15.output.dense.weight', 'text_encoder.bert.encoder.layer.15.output.dense.bias', 'text_encoder.bert.encoder.layer.15.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.15.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.16.attention.self.query.weight', 'text_encoder.bert.encoder.layer.16.attention.self.query.bias', 'text_encoder.bert.encoder.layer.16.attention.self.key.weight', 'text_encoder.bert.encoder.layer.16.attention.self.key.bias', 'text_encoder.bert.encoder.layer.16.attention.self.value.weight', 'text_encoder.bert.encoder.layer.16.attention.self.value.bias', 'text_encoder.bert.encoder.layer.16.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.16.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.16.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.16.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.16.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.16.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.16.output.dense.weight', 'text_encoder.bert.encoder.layer.16.output.dense.bias', 'text_encoder.bert.encoder.layer.16.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.16.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.17.attention.self.query.weight', 'text_encoder.bert.encoder.layer.17.attention.self.query.bias', 'text_encoder.bert.encoder.layer.17.attention.self.key.weight', 'text_encoder.bert.encoder.layer.17.attention.self.key.bias', 'text_encoder.bert.encoder.layer.17.attention.self.value.weight', 'text_encoder.bert.encoder.layer.17.attention.self.value.bias', 'text_encoder.bert.encoder.layer.17.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.17.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.17.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.17.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.17.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.17.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.17.output.dense.weight', 'text_encoder.bert.encoder.layer.17.output.dense.bias', 'text_encoder.bert.encoder.layer.17.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.17.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.18.attention.self.query.weight', 'text_encoder.bert.encoder.layer.18.attention.self.query.bias', 'text_encoder.bert.encoder.layer.18.attention.self.key.weight', 'text_encoder.bert.encoder.layer.18.attention.self.key.bias', 'text_encoder.bert.encoder.layer.18.attention.self.value.weight', 'text_encoder.bert.encoder.layer.18.attention.self.value.bias', 'text_encoder.bert.encoder.layer.18.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.18.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.18.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.18.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.18.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.18.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.18.output.dense.weight', 'text_encoder.bert.encoder.layer.18.output.dense.bias', 'text_encoder.bert.encoder.layer.18.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.18.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.19.attention.self.query.weight', 'text_encoder.bert.encoder.layer.19.attention.self.query.bias', 'text_encoder.bert.encoder.layer.19.attention.self.key.weight', 'text_encoder.bert.encoder.layer.19.attention.self.key.bias', 'text_encoder.bert.encoder.layer.19.attention.self.value.weight', 'text_encoder.bert.encoder.layer.19.attention.self.value.bias', 'text_encoder.bert.encoder.layer.19.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.19.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.19.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.19.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.19.crossattention.self.query.weight', 'text_encoder.bert.encoder.layer.19.crossattention.self.query.bias', 'text_encoder.bert.encoder.layer.19.crossattention.self.key.weight', 'text_encoder.bert.encoder.layer.19.crossattention.self.key.bias', 'text_encoder.bert.encoder.layer.19.crossattention.self.value.weight', 'text_encoder.bert.encoder.layer.19.crossattention.self.value.bias', 'text_encoder.bert.encoder.layer.19.crossattention.output.dense.weight', 'text_encoder.bert.encoder.layer.19.crossattention.output.dense.bias', 'text_encoder.bert.encoder.layer.19.crossattention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.19.crossattention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.19.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.19.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.19.output.dense.weight', 'text_encoder.bert.encoder.layer.19.output.dense.bias', 'text_encoder.bert.encoder.layer.19.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.19.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.20.attention.self.query.weight', 'text_encoder.bert.encoder.layer.20.attention.self.query.bias', 'text_encoder.bert.encoder.layer.20.attention.self.key.weight', 'text_encoder.bert.encoder.layer.20.attention.self.key.bias', 'text_encoder.bert.encoder.layer.20.attention.self.value.weight', 'text_encoder.bert.encoder.layer.20.attention.self.value.bias', 'text_encoder.bert.encoder.layer.20.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.20.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.20.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.20.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.20.crossattention.self.query.weight', 'text_encoder.bert.encoder.layer.20.crossattention.self.query.bias', 'text_encoder.bert.encoder.layer.20.crossattention.self.key.weight', 'text_encoder.bert.encoder.layer.20.crossattention.self.key.bias', 'text_encoder.bert.encoder.layer.20.crossattention.self.value.weight', 'text_encoder.bert.encoder.layer.20.crossattention.self.value.bias', 'text_encoder.bert.encoder.layer.20.crossattention.output.dense.weight', 'text_encoder.bert.encoder.layer.20.crossattention.output.dense.bias', 'text_encoder.bert.encoder.layer.20.crossattention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.20.crossattention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.20.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.20.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.20.output.dense.weight', 'text_encoder.bert.encoder.layer.20.output.dense.bias', 'text_encoder.bert.encoder.layer.20.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.20.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.21.attention.self.query.weight', 'text_encoder.bert.encoder.layer.21.attention.self.query.bias', 'text_encoder.bert.encoder.layer.21.attention.self.key.weight', 'text_encoder.bert.encoder.layer.21.attention.self.key.bias', 'text_encoder.bert.encoder.layer.21.attention.self.value.weight', 'text_encoder.bert.encoder.layer.21.attention.self.value.bias', 'text_encoder.bert.encoder.layer.21.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.21.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.21.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.21.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.21.crossattention.self.query.weight', 'text_encoder.bert.encoder.layer.21.crossattention.self.query.bias', 'text_encoder.bert.encoder.layer.21.crossattention.self.key.weight', 'text_encoder.bert.encoder.layer.21.crossattention.self.key.bias', 'text_encoder.bert.encoder.layer.21.crossattention.self.value.weight', 'text_encoder.bert.encoder.layer.21.crossattention.self.value.bias', 'text_encoder.bert.encoder.layer.21.crossattention.output.dense.weight', 'text_encoder.bert.encoder.layer.21.crossattention.output.dense.bias', 'text_encoder.bert.encoder.layer.21.crossattention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.21.crossattention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.21.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.21.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.21.output.dense.weight', 'text_encoder.bert.encoder.layer.21.output.dense.bias', 'text_encoder.bert.encoder.layer.21.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.21.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.22.attention.self.query.weight', 'text_encoder.bert.encoder.layer.22.attention.self.query.bias', 'text_encoder.bert.encoder.layer.22.attention.self.key.weight', 'text_encoder.bert.encoder.layer.22.attention.self.key.bias', 'text_encoder.bert.encoder.layer.22.attention.self.value.weight', 'text_encoder.bert.encoder.layer.22.attention.self.value.bias', 'text_encoder.bert.encoder.layer.22.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.22.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.22.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.22.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.22.crossattention.self.query.weight', 'text_encoder.bert.encoder.layer.22.crossattention.self.query.bias', 'text_encoder.bert.encoder.layer.22.crossattention.self.key.weight', 'text_encoder.bert.encoder.layer.22.crossattention.self.key.bias', 'text_encoder.bert.encoder.layer.22.crossattention.self.value.weight', 'text_encoder.bert.encoder.layer.22.crossattention.self.value.bias', 'text_encoder.bert.encoder.layer.22.crossattention.output.dense.weight', 'text_encoder.bert.encoder.layer.22.crossattention.output.dense.bias', 'text_encoder.bert.encoder.layer.22.crossattention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.22.crossattention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.22.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.22.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.22.output.dense.weight', 'text_encoder.bert.encoder.layer.22.output.dense.bias', 'text_encoder.bert.encoder.layer.22.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.22.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.23.attention.self.query.weight', 'text_encoder.bert.encoder.layer.23.attention.self.query.bias', 'text_encoder.bert.encoder.layer.23.attention.self.key.weight', 'text_encoder.bert.encoder.layer.23.attention.self.key.bias', 'text_encoder.bert.encoder.layer.23.attention.self.value.weight', 'text_encoder.bert.encoder.layer.23.attention.self.value.bias', 'text_encoder.bert.encoder.layer.23.attention.output.dense.weight', 'text_encoder.bert.encoder.layer.23.attention.output.dense.bias', 'text_encoder.bert.encoder.layer.23.attention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.23.attention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.23.crossattention.self.query.weight', 'text_encoder.bert.encoder.layer.23.crossattention.self.query.bias', 'text_encoder.bert.encoder.layer.23.crossattention.self.key.weight', 'text_encoder.bert.encoder.layer.23.crossattention.self.key.bias', 'text_encoder.bert.encoder.layer.23.crossattention.self.value.weight', 'text_encoder.bert.encoder.layer.23.crossattention.self.value.bias', 'text_encoder.bert.encoder.layer.23.crossattention.output.dense.weight', 'text_encoder.bert.encoder.layer.23.crossattention.output.dense.bias', 'text_encoder.bert.encoder.layer.23.crossattention.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.23.crossattention.output.LayerNorm.bias', 'text_encoder.bert.encoder.layer.23.intermediate.dense.weight', 'text_encoder.bert.encoder.layer.23.intermediate.dense.bias', 'text_encoder.bert.encoder.layer.23.output.dense.weight', 'text_encoder.bert.encoder.layer.23.output.dense.bias', 'text_encoder.bert.encoder.layer.23.output.LayerNorm.weight', 'text_encoder.bert.encoder.layer.23.output.LayerNorm.bias', 'text_encoder.cls.predictions.bias', 'text_encoder.cls.predictions.transform.dense.weight', 'text_encoder.cls.predictions.transform.dense.bias', 'text_encoder.cls.predictions.transform.LayerNorm.weight', 'text_encoder.cls.predictions.transform.LayerNorm.bias', 'text_encoder.cls.predictions.decoder.weight', 'text_encoder.cls.predictions.decoder.bias', 'text_proj.weight', 'text_proj.bias', 'itm_head.weight', 'itm_head.bias'])

------------------------------------- Calculate Flops Results -------------------------------------
Notations:
number of parameters (Params), number of multiply-accumulate operations(MACs),
number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),
fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),
default model backpropagation takes 2.00 times as much computation as forward propagation.

Total Training Params:                                                  0       
fwd MACs:                                                               22.08 TMACs
fwd FLOPs:                                                              44.19 TFLOPS
fwd+bwd MACs:                                                           66.25 TMACs
fwd+bwd FLOPs:                                                          132.56 TFLOPS

-------------------------------- Detailed Calculated FLOPs Results --------------------------------
Each module caculated is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). 
 They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.

VideoGPTPlusPhi3ForCausalLM(
  0 = 0% Params, 22.08 TMACs = 100% MACs, 44.19 TFLOPS = 100% FLOPs
  (model): VideoGPTPlusPhi3Model(
    0 = 0% Params, 21.75 TMACs = 98.47% MACs, 43.51 TFLOPS = 98.47% FLOPs
    (embed_tokens): Embedding(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 32011, 3072)
    (embed_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
    (layers): ModuleList(
      (0-31): 32 x Phi3DecoderLayer(
        0 = 0% Params, 460.72 GMACs = 2.09% MACs, 921.84 GFLOPS = 2.09% FLOPs
        (self_attn): Phi3Attention(
          0 = 0% Params, 201.76 GMACs = 0.91% MACs, 403.9 GFLOPS = 0.91% FLOPs
          (o_proj): Linear(0 = 0% Params, 32.37 GMACs = 0.15% MACs, 64.74 GFLOPS = 0.15% FLOPs, in_features=3072, out_features=3072, bias=False)
          (qkv_proj): Linear(0 = 0% Params, 97.11 GMACs = 0.44% MACs, 194.22 GFLOPS = 0.44% FLOPs, in_features=3072, out_features=9216, bias=False)
          (rotary_emb): Phi3RotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
        )
        (mlp): Phi3MLP(
          0 = 0% Params, 258.96 GMACs = 1.17% MACs, 517.94 GFLOPS = 1.17% FLOPs
          (gate_up_proj): Linear(0 = 0% Params, 172.64 GMACs = 0.78% MACs, 345.28 GFLOPS = 0.78% FLOPs, in_features=3072, out_features=16384, bias=False)
          (down_proj): Linear(0 = 0% Params, 86.32 GMACs = 0.39% MACs, 172.64 GFLOPS = 0.39% FLOPs, in_features=8192, out_features=3072, bias=False)
          (activation_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 28.1 MFLOPS = 0% FLOPs)
        )
        (input_layernorm): Phi3RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
        (resid_attn_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
        (resid_mlp_dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
        (post_attention_layernorm): Phi3RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
      )
    )
    (norm): Phi3RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
    (vision_tower): InternVideo2_Stage2V(
      0 = 0% Params, 4.04 TMACs = 18.28% MACs, 8.08 TFLOPS = 18.28% FLOPs
      (vision_encoder): PretrainInternVideo2(
        0 = 0% Params, 4.04 TMACs = 18.28% MACs, 8.08 TFLOPS = 18.28% FLOPs
        (patch_embed): PatchEmbed(
          0 = 0% Params, 3.39 GMACs = 0.02% MACs, 6.79 GFLOPS = 0.02% FLOPs
          (proj): Conv3d(0 = 0% Params, 3.39 GMACs = 0.02% MACs, 6.79 GFLOPS = 0.02% FLOPs, 3, 1408, kernel_size=(1, 14, 14), stride=(1, 14, 14))
          (norm): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
        )
        (blocks): ModuleList(
          (0): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
          )
          (1): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.006)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.006)
          )
          (2): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.013)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.013)
          )
          (3): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.019)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.019)
          )
          (4): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.026)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.026)
          )
          (5): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.032)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.032)
          )
          (6): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.038)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.038)
          )
          (7): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.045)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.045)
          )
          (8): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.051)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.051)
          )
          (9): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.058)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.058)
          )
          (10): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.064)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.064)
          )
          (11): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.070)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.070)
          )
          (12): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.077)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.077)
          )
          (13): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.083)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.083)
          )
          (14): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.090)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.090)
          )
          (15): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.096)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.096)
          )
          (16): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.103)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.103)
          )
          (17): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.109)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.109)
          )
          (18): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.115)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.115)
          )
          (19): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.122)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.122)
          )
          (20): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.128)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.128)
          )
          (21): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.135)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.135)
          )
          (22): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.141)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.141)
          )
          (23): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.147)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.147)
          )
          (24): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.154)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.154)
          )
          (25): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.160)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.160)
          )
          (26): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.167)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.167)
          )
          (27): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.173)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.173)
          )
          (28): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.179)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.179)
          )
          (29): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.186)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.186)
          )
          (30): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.192)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.192)
          )
          (31): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.199)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.199)
          )
          (32): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.205)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.205)
          )
          (33): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.212)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.212)
          )
          (34): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.218)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.218)
          )
          (35): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.224)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.224)
          )
          (36): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.231)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.231)
          )
          (37): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.237)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.237)
          )
          (38): Block(
            0 = 0% Params, 103.45 GMACs = 0.47% MACs, 206.92 GFLOPS = 0.47% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 32.51 GMACs = 0.15% MACs, 65.02 GFLOPS = 0.15% FLOPs
              (qkv): Linear(0 = 0% Params, 24.38 GMACs = 0.11% MACs, 48.77 GFLOPS = 0.11% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 8.13 GMACs = 0.04% MACs, 16.26 GFLOPS = 0.04% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.244)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 70.94 GMACs = 0.32% MACs, 141.9 GFLOPS = 0.32% FLOPs
              (fc1): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 25.19 MFLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 35.47 GMACs = 0.16% MACs, 70.94 GFLOPS = 0.16% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.244)
          )
          (39): Block(
            0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs
            (norm1): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (attn): Attention(
              0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs
              (qkv): Linear(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=1408, out_features=4224, bias=False)
              (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (proj): Linear(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=1408, out_features=1408, bias=True)
              (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (q_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
              (k_norm): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            )
            (ls1): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path1): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.250)
            (norm2): RMSNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (mlp): Mlp(
              0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs
              (fc1): Linear(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=1408, out_features=6144, bias=True)
              (act): GELU(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, approximate='none')
              (drop1): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
              (fc2): Linear(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=6144, out_features=1408, bias=True)
              (drop2): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            )
            (ls2): LayerScale(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
            (drop_path2): DropPath(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, drop_prob=0.250)
          )
        )
        (clip_projector): AttentionPoolingBlock(
          0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs
          (norm1_q): LayerNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, (1408,), eps=1e-05, elementwise_affine=True)
          (norm1_k): LayerNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, (1408,), eps=1e-05, elementwise_affine=True)
          (norm1_v): LayerNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, (1408,), eps=1e-05, elementwise_affine=True)
          (cross_attn): CrossAttention(
            0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs
            (q): Linear(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=1408, out_features=1408, bias=False)
            (k): Linear(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=1408, out_features=1408, bias=False)
            (v): Linear(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=1408, out_features=1408, bias=False)
            (attn_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
            (proj): Linear(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=1408, out_features=768, bias=True)
            (proj_drop): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.0, inplace=False)
          )
          (drop_path): Identity(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
        )
        (clip_decoder): ModuleList(
          (0-5): 6 x Linear_Decoder(
            0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs
            (head): Linear(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=1408, out_features=3200, bias=True)
            (norm): LayerNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, (3200,), eps=1e-05, elementwise_affine=True)
          )
        )
        (final_clip_decoder): Linear_Decoder(
          0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs
          (head): Linear(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=768, bias=True)
          (norm): LayerNorm(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, (768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (vision_proj): Linear(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, in_features=768, out_features=512, bias=True)
    )
    (image_vision_tower): CLIPVisionTower(
      0 = 0% Params, 2.79 TMACs = 12.65% MACs, 5.59 TFLOPS = 12.65% FLOPs
      (vision_tower): CLIPVisionModel(
        0 = 0% Params, 2.79 TMACs = 12.65% MACs, 5.59 TFLOPS = 12.65% FLOPs
        (vision_model): CLIPVisionTransformer(
          0 = 0% Params, 2.79 TMACs = 12.65% MACs, 5.59 TFLOPS = 12.65% FLOPs
          (embeddings): CLIPVisionEmbeddings(
            0 = 0% Params, 5.55 GMACs = 0.03% MACs, 11.1 GFLOPS = 0.03% FLOPs
            (patch_embedding): Conv2d(0 = 0% Params, 5.55 GMACs = 0.03% MACs, 11.1 GFLOPS = 0.03% FLOPs, 3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 577, 1024)
          )
          (pre_layrnorm): LayerNorm(0 = 0% Params, 0 MACs = 0% MACs, 47.27 MFLOPS = 0% FLOPs, (1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            0 = 0% Params, 2.79 TMACs = 12.62% MACs, 5.58 TFLOPS = 12.63% FLOPs
            (layers): ModuleList(
              (0-23): 24 x CLIPEncoderLayer(
                0 = 0% Params, 116.17 GMACs = 0.53% MACs, 232.51 GFLOPS = 0.53% FLOPs
                (self_attn): CLIPAttention(
                  0 = 0% Params, 38.72 GMACs = 0.18% MACs, 77.53 GFLOPS = 0.18% FLOPs
                  (k_proj): Linear(0 = 0% Params, 9.68 GMACs = 0.04% MACs, 19.36 GFLOPS = 0.04% FLOPs, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(0 = 0% Params, 9.68 GMACs = 0.04% MACs, 19.36 GFLOPS = 0.04% FLOPs, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(0 = 0% Params, 9.68 GMACs = 0.04% MACs, 19.36 GFLOPS = 0.04% FLOPs, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(0 = 0% Params, 9.68 GMACs = 0.04% MACs, 19.36 GFLOPS = 0.04% FLOPs, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(0 = 0% Params, 0 MACs = 0% MACs, 47.27 MFLOPS = 0% FLOPs, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  0 = 0% Params, 77.44 GMACs = 0.35% MACs, 154.89 GFLOPS = 0.35% FLOPs
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)
                  (fc1): Linear(0 = 0% Params, 38.72 GMACs = 0.18% MACs, 77.44 GFLOPS = 0.18% FLOPs, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(0 = 0% Params, 38.72 GMACs = 0.18% MACs, 77.44 GFLOPS = 0.18% FLOPs, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(0 = 0% Params, 0 MACs = 0% MACs, 47.27 MFLOPS = 0% FLOPs, (1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm(0 = 0% Params, 0 MACs = 0% MACs, 81.92 KFLOPS = 0% FLOPs, (1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      0 = 0% Params, 56.37 GMACs = 0.26% MACs, 112.76 GFLOPS = 0.26% FLOPs
      (0): Linear(0 = 0% Params, 17.72 GMACs = 0.08% MACs, 35.43 GFLOPS = 0.08% FLOPs, in_features=1408, out_features=3072, bias=True)
      (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 12.58 MFLOPS = 0% FLOPs, approximate='none')
      (2): Linear(0 = 0% Params, 38.65 GMACs = 0.18% MACs, 77.31 GFLOPS = 0.17% FLOPs, in_features=3072, out_features=3072, bias=True)
    )
    (image_mm_projector): Sequential(
      0 = 0% Params, 115.96 GMACs = 0.53% MACs, 231.96 GFLOPS = 0.52% FLOPs
      (0): Linear(0 = 0% Params, 28.99 GMACs = 0.13% MACs, 57.98 GFLOPS = 0.13% FLOPs, in_features=1024, out_features=3072, bias=True)
      (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 28.31 MFLOPS = 0% FLOPs, approximate='none')
      (2): Linear(0 = 0% Params, 86.97 GMACs = 0.39% MACs, 173.95 GFLOPS = 0.39% FLOPs, in_features=3072, out_features=3072, bias=True)
    )
  )
  (lm_head): Linear(0 = 0% Params, 337.3 GMACs = 1.53% MACs, 674.6 GFLOPS = 1.53% FLOPs, in_features=3072, out_features=32011, bias=False)
)
---------------------------------------------------------------------------------------------------
[Our Model] FLOPs:44.19 TFLOPS   MACs:22.08 TMACs   Params:0 

> /hdd2/wxc/VideoGPT-plus/eval/mvbench/inference/infer.py(123)eval_model()
-> with torch.inference_mode():
(Pdb) --KeyboardInterrupt--
(Pdb) --KeyboardInterrupt--
(Pdb) --KeyboardInterrupt--
(Pdb) --KeyboardInterrupt--
(Pdb) --KeyboardInterrupt--
(Pdb) 